{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from Databricks#\n",
    "df=spark.read.format(file_type)\\ #csv,txt,json...\n",
    "    .option('multiline','true')\\ #is there multiple rows or not\n",
    "    .option('inferSchema','true')\\ #whether machine needs to automatically detect the schema of data or not\n",
    "    .option('header','true')\\ \n",
    "    .load(file_location) #location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from DBFS to dataframe#\n",
    "df.write.format(file_type)\\\n",
    "    .option('header','true')\\\n",
    "    .option('delimiter',',')\\ #store the file, for each row we have several features and separated by comma\n",
    "    .save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple manipulation in dataframe\n",
    "df.cache()\n",
    "display(df)\n",
    "df.printSchema()\n",
    "df.columns #get the name of each column\n",
    "df.describe().show() #statistic summary of numeric column\n",
    "type(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[“column1”...”columnN”]] #create a new df with N columns, we write two bricks\n",
    "df.withColumnRenamed(“old column name”, “new column name”)\n",
    "df.withColumn(“column name”, df.[“column name”].cast(“data type”)) #create a new df assigning the data type for the column\n",
    "df.withColumn(“new column name”, lit(default value)) #create a new df adding new columns with a default value\n",
    "df.drop(“column name”) \n",
    "df.createOrReplaceTempView(“SQL temporary view name”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "#Simple SQL Manipulations#\n",
    "%sql \n",
    "select columnname from tablename where conditions\n",
    "--column name does not include a space, we use _ to connect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD#\n",
    "#create RDD from external dataset file\n",
    "sc = SparkContext(“local”, “texfile”)\n",
    "rdd_new = sc.textFile(“/mnt/S3data/sample.text”)\n",
    "#create RDD from existing dataframe\n",
    "rdd_new = df.rdd\n",
    "#convert RDD to dataframe\n",
    "rdd_new.toDF()\n",
    "\n",
    "#python function#\n",
    "#1.\n",
    "def fun_name(input):\n",
    "    return output\n",
    "#2.\n",
    "lambda input: output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform RDD:\n",
    "#Filter RDD: select some data in RDD to create a new RDD,similar to select in SQL\n",
    "rdd.filter(lambda x: x[0]==`John’)\n",
    "rdd.filter(lambda x: x[1][0]<=20) \n",
    "#Map:apply a function to each data in RDD\n",
    "rdd.map(lambda x: (x[0], x[1][1]*0.393))\n",
    "#union,join\n",
    "\n",
    "#Actions in RDD:\n",
    "collect\n",
    "take(n)\n",
    "count\n",
    "reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas Dataframe#\n",
    "import pandas as pd\n",
    "pd_new = df.toPandas()\n",
    "pd.head(2)\n",
    "df.tail(2)\n",
    "pd.columns\n",
    "pd.describe()\n",
    "pd[[‘column1’, ‘column2’]]\n",
    "pd[0:3] #select rows\n",
    "pd.loc[0,’column’]\n",
    "pd.iloc[0,1]\n",
    "pd[‘colum1’,’colum2’].apply(fun)\n",
    "pd.merge(pd1, pd2, on=‘column’)\n",
    "pd.concat([pd1,pd2])\n",
    "pd.dropna(axis=0, inplace=True)\n",
    "pd.drop_duplicates(subset=‘column’, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy#\n",
    "np.array([[1,2,3],[4,5,6]])\n",
    "np1 = pd.to_numpy() #conversion from pandas dataframe\n",
    "np.zeros((2,2))\n",
    "np.ones((2,2))\n",
    "np.arange(0,3,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear Regression with MLib#\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "From pyspark.ml.linalg import Vectors\n",
    "# add a new column (feature column) to the original dataframe \n",
    "assembler = VectorAssembler (inputCols=[colum1, colum2, ...], outputCol=“features”)\n",
    "new_df = assembler.transform(old_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training#\n",
    "train_dt, test_dt = df.randomSplit([0.7,0.3])\n",
    "#run linear regression to estimate the parameters\n",
    "lr = LinearRegression(labelCol=labelcolumname, featuresCol=featurecolumname)\n",
    "#fit the model\n",
    "lrmodel = lr.fit(train_dt)\n",
    "#check the results:\n",
    "lrmodel.intercept\n",
    "lrmodel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Testing#\n",
    "test_results = lrmodel.evaluate(test_dt)\n",
    "#check the testing result:\n",
    "test_results.residuals\n",
    "test_results.summary.meanSquaredError\n",
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Linear Model#\n",
    "train_dt, test_dt = df.randomSplit([0.7,0.3])\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression(family=distribution type, link=link function type)\n",
    "#for example:\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "glrmodel = glr.fit(train_dt)\n",
    "#training result using training dataset\n",
    "glrmodel.intercept\n",
    "glrmodel.coefficients\n",
    "glrmodel.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()\n",
    "\n",
    "#evaluate the model on test dataset\n",
    "test_results = model.evaluate(test_dt)\n",
    "type(test_results)\n",
    "test_results.residuals().show()\n",
    "test_results.residuals().columns\n",
    "test_results.residuals().createOrReplaceGlobalTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree method#\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier\n",
    "#run decision tree models to estimate the parameters\n",
    "dtc = DecisionTreeClassifier(labelCol=‘column’,featuresCol='column vector')\n",
    "rfc = RandomForestClassifier(labelCol=‘column',featuresCol='column vector')\n",
    "dtc_model = dtc.fit(train_dt)\n",
    "rfc_model = rfc.fit(train_dt)\n",
    "#test our model on test dataset, model comparison\n",
    "dtc_model.transform(test_df)\n",
    "rfc_model.transform(test_df)\n",
    "\n",
    "#evaluation metrices\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"Private\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "#print the results\n",
    "print(\"Here are the results!\")\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost#\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 0)\n",
    "from sparkdl.xgboost import XgboostRegressor\n",
    "xgb_regressor = XgboostRegressor(num_workers=3, labelCol=\"cnt\", missing=0.0)\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: maximum depth of each decision tree \n",
    "#  - maxIter: iterations, or the total number of trees \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(xgb_regressor.max_depth, [2, 5])\\\n",
    "  .addGrid(xgb_regressor.n_estimators, [10, 100])\\\n",
    "  .build()\n",
    "# Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=xgb_regressor.getLabelCol(),\n",
    "                                predictionCol=xgb_regressor.getPredictionCol())\n",
    "# Declare the CrossValidator, which performs the model tuning.\n",
    "cv = CrossValidator(estimator=xgb_regressor, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNN#\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "layers = [#input, #hidden1,..., #hidden K, #output]\n",
    "# create the trainer and set its parameters,maxIter is the gradient desent parameter.\n",
    "fnn = MultilayerPerceptronClassifier(maxIter=100, layers=layers, seed=1234)\n",
    "# train the model\n",
    "fnn_model = fnn.fit(train_df)\n",
    "fnn_model.transform(test_df)\n",
    "# compute accuracy on the test set\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction to Keras#\n",
    "#Build a softmax classifier with one layer#\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model=model.Sequential()\n",
    "model.add(layers.Dense(10,activation='softmax',imput_shape=(784,)))\n",
    "model.summary()\n",
    "\n",
    "#train classifer#speficy optimzer, learning rate, loss function, metric\n",
    "from keras import optimizers\n",
    "model.compile(optimizers.RMSprop(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#specify batch size & # of epoch：One pass over the n training samples.\n",
    "history=model.fit(x_train_vec,y_train_vec,\n",
    "                 batch_size=128,epochs=50,\n",
    "                 validation_data=(x_valid_vec,y_valid_vec))\n",
    "#evaluation results\n",
    "loss_and_acc=model.evaluate(x_test_vec,y_test_vec)\n",
    "print('loss=' +str(loss_and_acc[0]))\n",
    "print('accuracy=' +str(loss_and_acc[1]))\n",
    "\n",
    "\n",
    "#Build Full connected Netrual Network with 2-layers#\n",
    "from keras import models \n",
    "from keras import layers \n",
    "d1 = 500 # width of the 1st hidden layer d1 = 500 \n",
    "d2 = 500 # width of the 2nd hidden layer d2 = 500\n",
    "model = models.Sequential() \n",
    "model.add(layers.Dense(d1, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(d2, activation='relu')) \n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "model.summary()\n",
    "#train classifer\n",
    "from keras import optimizers\n",
    "model.compile(optimizers.RMSprop(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#fit the model\n",
    "history=model.fit(x_train_vec, y_train_vec,\n",
    "                  batch_size=128,epochs=50,\n",
    "                  validation_data=(x_valid_vec,y_valid_vec))\n",
    "#evaluation results\n",
    "loss_and_acc=model.evaluate(x_test_vec,y_test_vec)\n",
    "print('loss=' +str(loss_and_acc[0]))\n",
    "print('accuracy=' +str(loss_and_acc[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17245beed66d97676295f18f5af02f52c1ff0b20014505018e20bb50c7c46d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
