{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-Convolutional Neural Network\n",
    "# Usually used for image processing,get the partial correlation of image features\n",
    "# Structure of CNN:input layer(3D)->Feature Learning:Convolution & ReLU, pooling, Convolution & ReLU, pooling->Classification(like FNN):flatten->fully connected->softmax\n",
    "#convolution filter:higher convolution, more overlaps between f and g\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential()\n",
    "keras.Input(shape=input_shape), #we state the input shape before\n",
    "layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), \n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Flatten(),   #vectorization\n",
    "layers.Dropout(0.5), #randomly remove some results in order to avoid overfitting\n",
    "layers.Dense(num_classes, activation='softmax')])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=15)\n",
    "score = model.evaluate(x_test,y_test)\n",
    "score[0]: loss\n",
    "score[1]: accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(10, (5, 5), activation='relu', input_shape=(28, 28, 1))) #10->filter number\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(20, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nerual Network Common Tricks#\n",
    "#drop out when training the model(1 backward & 1 forword); While in prediction, there is no dropout, using all parameters\n",
    "#Process: randomly choose nearly 50% of neurons->remove the selected ones->multiply the unselected neurons by 1/0.5=2\n",
    "model = models.Sequential() \n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3))) \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5)) #before the first dense layer for regulization\n",
    "model.add(layers.Dense(512, activation='relu')) \n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "#make the decision base on part of the features\n",
    "#drop out is a kind of regularization\n",
    "\n",
    "#Data Augmentation#\n",
    "#generate more training samples from existing training data\n",
    "#method:flip, rotation, crop, shift, adding random noise\n",
    "\n",
    "#Pretrain#\n",
    "#pretrain a deep net on large-scale dataset\n",
    "#remove the top layers, build new top layers,freeze the base layers, train the top layers\n",
    "#Optimal:fine-tune top Conv layers\n",
    "#Conv layers: for feature extraction. Low level are more effective to high level features. Less trainable parameters, less prone to overfitting.\n",
    "\n",
    "#Ensemble#\n",
    "#Bagging with different models&predictions-->vote\n",
    "#Varying from different network structures/random initalizations/optimization algorithms\n",
    "#reduce variance\n",
    "\n",
    "#Multi-Task Learning#\n",
    "#set hyperparameters for loss objective funtions\n",
    "\n",
    "#Feature Scaling#\n",
    "#standardization~distribution(0,1),get the Hessian matrix of linear regression\n",
    "\n",
    "#Batch Normalization# standardization of Hidden Layer\n",
    "from keras import models from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(10, (5, 5), input_shape=(28, 28, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Conv2D(20, (5, 5)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu')) \n",
    "model.add(layers.MaxPooling2D((2, 2))) \n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu')) \n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Processing#\n",
    "#1.Tokenization:breaks a piece of text down into a list of tokens. Token is a word. Lower/Upper case, Remove the stop words, Typo correction\n",
    "#2.Build Dictionary:hash table for counting words frequency, map words to index, a list of indices forms a sequence\n",
    "#3.One-hot Encoding\n",
    "#4.Align Sequences: the paragraph we train have different lengths w_i.Solutions: cut off the text to keep w words; if the length<7,we add up zeros up to w\n",
    "#5.Word Embedding:one-hot vector->low dimensional vectors.After that, words' cosine similarities are very close\n",
    "from ctypes.wintypes import WORD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import preprocessing\n",
    "vocabulary=10000\n",
    "tokenizer=Tokenizer(num_words=vocabulary)\n",
    "tokenizer.fit_on_texts(texts_train) #import the dataframe\n",
    "\n",
    "word_index=tokenizer.word_index #build the dictionary\n",
    "sequences_train=tokenizer.text_to_sequences(texts_train) #encoding to a sequence\n",
    "\n",
    "print(sequences_train[0]) \n",
    "\n",
    "word_num=20 #the length of each sequence is equal\n",
    "x_train=preprocessing.sequence.pad_sequences(sequences_train,maxlen=word_num) #align sequence\n",
    "\n",
    "x_train.shape\n",
    "x_train[0]\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "embedding_dim=8\n",
    "model.add(Embedding(vocabulary,embedding_dim,input_length=word_num))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "from keras import optimizers\n",
    "epochs=50\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.0001),loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train, epochs=epochs,batch_size=32,validation_data=(x_valid,y_vaild))\n",
    "\n",
    "loss_and_acc=model.evaluate(x_test,labels_test)\n",
    "print('loss= '+str(loss_and_acc[0]))\n",
    "print('acc= '+str(loss_and_acc[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17245beed66d97676295f18f5af02f52c1ff0b20014505018e20bb50c7c46d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


